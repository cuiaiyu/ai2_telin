physicaliqa:
  roberta:
    roberta-large:
      lr: 5e-6
      batch_size: 8
    large_roberta:
      lr: 5e-6
      batch_size: 4
    lm_with_dp_graphs_gtn1L_fcn_dpbatch1_multitask_beta_0p001_10000:
      lr: 5e-6
      batch_size: 4
      dropout: 0.1
      accumulate_grad_batches: 2
      max_nb_epochs: 4
    lm_with_dp_graphs_gtn1L_fcn_dpbatch1_multitask_beta_0p001_10000_bc:
      lr: 5e-6
      batch_size: 4
      dropout: 0
      accumulate_grad_batches: 2
      max_nb_epochs: 4
    lm_with_dp_graphs_gtn1L_fcn_dpbatch1_multitask_beta_0p001_10000_bc2:
      lr: 1e-6
      batch_size: 4
      dropout: 0
      accumulate_grad_batches: 2
      max_nb_epochs: 5
    lm_with_dp_graphs_gtn1L_fcn_dpbatch1_multitask_beta_0p001_12000:
      lr: 5e-6
      batch_size: 4
      dropout: 0.1
      accumulate_grad_batches: 2
      max_nb_epochs: 6
    mutimodal_lm_large_finetune_cc_cn_dc_30000:
      lr: 5e-6
      batch_size: 4
      dropout: 0.1
      accumulate_grad_batches: 4
    baseline_w_cn_all_cs_v1:
      lr: 5e-6
      batch_size: 4
      dropout: 0.1
      accumulate_grad_batches: 2
  default:
    seed: 42
    lr: 2e-5
    dropout: 0.5
    batch_size: 32
    max_seq_len: 128
    max_nb_epochs: 3
    initializer_range: 0.02
    weight_decay: 0.0
    warmup_steps: 0
    adam_epsilon: 1e-8
    accumulate_grad_batches: 1
    do_lower_case: false
